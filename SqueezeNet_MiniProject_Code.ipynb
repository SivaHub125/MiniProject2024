{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "O5EhY0xS_v_K",
        "G6om8_bl_SB5",
        "3Ypps7t1_ZXa",
        "AI9z20AA-EuS",
        "5jzD2kzc-Lfx",
        "RwNpbMlR-Pb_",
        "BeXebPLlFcsJ",
        "fbkhgCraFwvT",
        "tlolBK6HFYT9",
        "xXpw6BgkF5KE",
        "PFZesfVB2u19"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Mounting GDrive"
      ],
      "metadata": {
        "id": "O5EhY0xS_v_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOr9gL58O6rl",
        "outputId": "2de1f201-b063-4335-eb09-c271e33a184d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cropping Image"
      ],
      "metadata": {
        "id": "G6om8_bl_SB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "#function for cropping images\n",
        "def cropImg(folder_path,output_folder):\n",
        "\n",
        "    image_files = [f for f in os.listdir(folder_path)]\n",
        "    left = 100\n",
        "    top = 300\n",
        "    right = 2125\n",
        "    bottom = 1480\n",
        "    for image in image_files:\n",
        "            # Open the image file\n",
        "            image_path = os.path.join(folder_path, image)\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "            # Crop the image using the specified dimensions\n",
        "            cropped_img = img.crop((left, top, right, bottom))\n",
        "\n",
        "            # Save the cropped image to the output folder\n",
        "            output_path = os.path.join(output_folder, f\"{image}\")\n",
        "            cropped_img.save(output_path)"
      ],
      "metadata": {
        "id": "cDw8McL1_RDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path1=\"/content/drive/MyDrive/MP_Dataset/ECG Images of Myocardial Infarction Patients (240x12=2880)\"\n",
        "output_folder1=\"/content/drive/MyDrive/MP_Dataset/CroppedMI\"\n",
        "cropImg(folder_path1,output_folder1)\n",
        "\n",
        "folder_path2=\"/content/drive/MyDrive/MP_Dataset/ECG Images of Patient that have abnormal heartbeat (233x12=2796)\"\n",
        "output_folder2=\"/content/drive/MyDrive/MP_Dataset/CroppedAB\"\n",
        "cropImg(folder_path2,output_folder2)\n",
        "\n",
        "folder_path3=\"/content/drive/MyDrive/MP_Dataset/ECG Images of Patient that have History of MI (172x12=2064)\"\n",
        "output_folder3=\"/content/drive/MyDrive/MP_Dataset/CroppedHMI\"\n",
        "cropImg(folder_path3,output_folder3)\n",
        "\n",
        "folder_path4=\"/content/drive/MyDrive/MP_Dataset/Normal Person ECG Images (284x12=3408)\"\n",
        "output_folder4=\"/content/drive/MyDrive/MP_Dataset/CroppedN\"\n",
        "cropImg(folder_path4,output_folder4)"
      ],
      "metadata": {
        "id": "Zswnr_5z_Wr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agumentation is performed"
      ],
      "metadata": {
        "id": "3Ypps7t1_ZXa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayeBT3OoEmnG",
        "outputId": "b00cddbf-e9cb-42fb-e095-45b83bf6ecec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images =  239\n",
            "Number of images =  172\n",
            "Number of images =  284\n",
            "Number of images =  233\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "    rows, cols, _ = image.shape\n",
        "    rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, (cols, rows))\n",
        "    return rotated_image\n",
        "\n",
        "def flip_image(image, flip_code):\n",
        "    flipped_image = cv2.flip(image, flip_code)\n",
        "    return flipped_image\n",
        "\n",
        "def translate_image(image, tx, ty):\n",
        "    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    translated_image = cv2.warpAffine(image, translation_matrix, (image.shape[1], image.shape[0]))\n",
        "    return translated_image\n",
        "\n",
        "def apply_augmentation(input_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Get all ECG image files in the input folder\n",
        "    ecg_files = [f for f in os.listdir(input_folder)]\n",
        "    print(\"Number of images = \",len(ecg_files))\n",
        "\n",
        "    for ecg_file in ecg_files:\n",
        "        # Read the ECG image\n",
        "        ecg_path = os.path.join(input_folder, ecg_file)\n",
        "        ecg_image = cv2.imread(ecg_path)\n",
        "\n",
        "        # Apply data augmentation\n",
        "\n",
        "        rotated_image = rotate_image(ecg_image, angle=1)\n",
        "        rotated_image1 = rotate_image(ecg_image, angle=-1)\n",
        "\n",
        "        translation_x, translation_y = 1, 1\n",
        "        translated_image = translate_image(ecg_image, translation_x, translation_y)\n",
        "        translated_image1 = translate_image(rotated_image, translation_x, translation_y)\n",
        "\n",
        "        # Save the augmented image\n",
        "        output_path = os.path.join(output_folder, f\"{ecg_file}\")\n",
        "        cv2.imwrite(output_path, ecg_image)\n",
        "\n",
        "        output_path = os.path.join(output_folder, f\"Trans{ecg_file}\")\n",
        "        cv2.imwrite(output_path, translated_image)\n",
        "\n",
        "        output_path = os.path.join(output_folder, f\"Rot{ecg_file}\")\n",
        "        cv2.imwrite(output_path, rotated_image)\n",
        "\n",
        "        output_path = os.path.join(output_folder, f\"tran{ecg_file}\")\n",
        "        cv2.imwrite(output_path, translated_image1)\n",
        "\n",
        "        output_path = os.path.join(output_folder, f\"ro{ecg_file}\")\n",
        "        cv2.imwrite(output_path, rotated_image1)\n",
        "\n",
        "        if(output_folder==\"Dataset/AugmentedHMIPatient\"):\n",
        "            translated_image2 = translate_image(rotated_image1, translation_x, translation_y)\n",
        "            translation_x, translation_y = 1.5, 1.5\n",
        "            translated_image3 = translate_image(ecg_image, translation_x, translation_y)\n",
        "\n",
        "            output_path = os.path.join(output_folder, f\"rotT{ecg_file}\")\n",
        "            cv2.imwrite(output_path, translated_image2)\n",
        "\n",
        "            output_path = os.path.join(output_folder, f\"Trt{ecg_file}\")\n",
        "            cv2.imwrite(output_path, translated_image3)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the input folder and output folder\n",
        "    input_folder=\"/content/drive/MyDrive/MP_Dataset/CroppedMI\"\n",
        "    output_folder=\"/content/drive/MyDrive/MP_Dataset/AugmentedMI\"\n",
        "    apply_augmentation(input_folder, output_folder)\n",
        "\n",
        "    input_folder=\"/content/drive/MyDrive/MP_Dataset/CroppedHMI\"\n",
        "    output_folder=\"/content/drive/MyDrive/MP_Dataset/AugmentedHMI\"\n",
        "    apply_augmentation(input_folder, output_folder)\n",
        "\n",
        "    input_folder=\"/content/drive/MyDrive/MP_Dataset/CroppedN\"\n",
        "    output_folder=\"/content/drive/MyDrive/MP_Dataset/AugmentedN\"\n",
        "    apply_augmentation(input_folder, output_folder)\n",
        "\n",
        "    input_folder=\"/content/drive/MyDrive/MP_Dataset/CroppedAB\"\n",
        "    output_folder=\"/content/drive/MyDrive/MP_Dataset/AugmentedAB\"\n",
        "    apply_augmentation(input_folder, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Adding the Image into Separate Array"
      ],
      "metadata": {
        "id": "AI9z20AA-EuS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl1w2oDbEqjy",
        "outputId": "25c1dfdc-2696-4735-9e24-f43ddd7a7edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECG of normal persons = 1200\n",
            "ECG of abnormal persons 1165\n",
            "ECG of MI persons = 1195\n",
            "ECG of HMI persons = 1140\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "normal_path= [\n",
        "    os.path.join(os.getcwd(), \"/content/drive/MyDrive/MP_Dataset/AugmentedN\", x)\n",
        "    for x in os.listdir(\"/content/drive/MyDrive/MP_Dataset/AugmentedN\")\n",
        "]\n",
        "abnormal_path= [\n",
        "    os.path.join(os.getcwd(), \"/content/drive/MyDrive/MP_Dataset/AugmentedAB\", x)\n",
        "    for x in os.listdir(\"/content/drive/MyDrive/MP_Dataset/AugmentedAB\")\n",
        "]\n",
        "Mi_path= [\n",
        "    os.path.join(os.getcwd(), \"/content/drive/MyDrive/MP_Dataset/AugmentedMI\", x)\n",
        "    for x in os.listdir(\"/content/drive/MyDrive/MP_Dataset/AugmentedMI\")\n",
        "]\n",
        "Hmi_path= [\n",
        "    os.path.join(os.getcwd(), \"/content/drive/MyDrive/MP_Dataset/AugmentedHMI\", x)\n",
        "    for x in os.listdir(\"/content/drive/MyDrive/MP_Dataset/AugmentedHMI\")\n",
        "]\n",
        "\n",
        "print(\"ECG of normal persons = \" + str(len(normal_path)))\n",
        "print(\"ECG of abnormal persons \" + str(len(abnormal_path)))\n",
        "print(\"ECG of MI persons = \" + str(len(Mi_path)))\n",
        "print(\"ECG of HMI persons = \" + str(len(Hmi_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assigning Labels"
      ],
      "metadata": {
        "id": "5jzD2kzc-Lfx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxPUc2jhEstr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "abnormal_labels = np.array([0 for _ in range(len(abnormal_path))])\n",
        "normal_labels = np.array([1 for _ in range(len(normal_path))])\n",
        "mi_labels= np.array([2 for _ in range(len(Mi_path))])\n",
        "hmi_labels= np.array([3 for _ in range(len(Hmi_path))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Images and Labels as separate arrays"
      ],
      "metadata": {
        "id": "RwNpbMlR-Pb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9mtDzD3EvVk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine image paths and labels into tuples\n",
        "abnormal_data = list(zip(abnormal_path, abnormal_labels))\n",
        "normal_data = list(zip(normal_path, normal_labels))\n",
        "mi_data = list(zip(Mi_path, mi_labels))\n",
        "hmi_data = list(zip(Hmi_path, hmi_labels))\n",
        "\n",
        "data=[]\n",
        "data.extend(abnormal_data)\n",
        "data.extend(normal_data)\n",
        "data.extend(mi_data)\n",
        "data.extend(hmi_data)\n",
        "\n",
        "X_paths = np.array([image_path for image_path, _ in data])\n",
        "y = np.array([label for _, label in data])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SqueezeNet Model BuiltIn Function"
      ],
      "metadata": {
        "id": "BeXebPLlFcsJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KafZha5XO2tH",
        "outputId": "34d52fd0-1cb7-47e5-f43a-1ff01fa19845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
            "100%|██████████| 4.78M/4.78M [00:00<00:00, 99.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 1248424\n",
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load the pre-trained SqueezeNet model\n",
        "model1 = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
        "total_params = sum(p.numel() for p in model1.parameters())\n",
        "print(\"Total number of parameters:\", total_params)\n",
        "print(model1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for SqueezeNet Model"
      ],
      "metadata": {
        "id": "fbkhgCraFwvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Fire(nn.Module):\n",
        "    def __init__(self, inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "            Fire(128, 16, 64, 64),\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(256, 32, 128, 128),\n",
        "            Fire(256, 48, 192, 192),\n",
        "            Fire(384, 48, 192, 192),\n",
        "            Fire(384, 64, 256, 256),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(512, 64, 256, 256)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Conv2d(512, 1000, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "# Instantiate the model\n",
        "model = SqueezeNet()\n",
        "total_params = sum(p.numel() for p in model1.parameters())\n",
        "print(\"Total number of parameters:\", total_params)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjsMUhN0PY6p",
        "outputId": "432435f8-ae11-4398-dfee-bf9bfeb02019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 1248424\n",
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR 0.001"
      ],
      "metadata": {
        "id": "tlolBK6HFYT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your SqueezeNet model class here...\n",
        "\n",
        "# Define your dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X_paths, y, transform=None):\n",
        "        self.X_paths = X_paths\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.X_paths[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        label = self.y[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Split data into train and test\n",
        "X_paths = np.array([image_path for image_path, _ in data])\n",
        "y = np.array([label for _, label in data])\n",
        "# Split data into train and test\n",
        "# Assume you have already split the data into train and test sets\n",
        "X_train_paths, X_test_paths, y_train, y_test = train_test_split(X_paths, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = CustomDataset(X_train_paths, y_train, transform=transform)\n",
        "test_dataset = CustomDataset(X_test_paths, y_test, transform=transform)\n",
        "\n",
        "# Define dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "avg_accuracy = correct / total\n",
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'squeezenet_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WIgefgbQMpm",
        "outputId": "1e08531d-01e6-4d0c-b129-67ab2ad15cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.9737\n",
            "Epoch [2/10], Loss: 1.4285\n",
            "Epoch [3/10], Loss: 1.4265\n",
            "Epoch [4/10], Loss: 1.4181\n",
            "Epoch [5/10], Loss: 1.4114\n",
            "Epoch [6/10], Loss: 1.4087\n",
            "Epoch [7/10], Loss: 1.4175\n",
            "Epoch [8/10], Loss: 1.4146\n",
            "Epoch [9/10], Loss: 1.4096\n",
            "Epoch [10/10], Loss: 1.4085\n",
            "Average Accuracy: 0.2383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction of class (SqueezeNet LR 0.001) Acurracy 0.24%"
      ],
      "metadata": {
        "id": "xXpw6BgkF5KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define transforms for individual images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the saved model\n",
        "model = SqueezeNet()\n",
        "model.load_state_dict(torch.load('squeezenet_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to predict the class of an individual image\n",
        "def predict_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/content/drive/MyDrive/MP_Dataset/AugmentedHMI/Copy of PMI(10).jpg\"\n",
        "predicted_class = predict_image(image_path)\n",
        "print(\"Predicted class:\", predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8_Ylj0r4l4n",
        "outputId": "a3dee23b-70c1-4137-eb97-7de7229a9e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define transforms for individual images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the saved model\n",
        "model = SqueezeNet()\n",
        "model.load_state_dict(torch.load('squeezenet_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to predict the class of an individual image\n",
        "def predict_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/content/drive/MyDrive/MP_Dataset/AugmentedN/Normal(101).jpg\"\n",
        "predicted_class = predict_image(image_path)\n",
        "print(\"Predicted class:\", predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dveF1_cRagDf",
        "outputId": "de515756-ec74-4ee0-bac9-47fce2e5207e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define transforms for individual images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the saved model\n",
        "model = SqueezeNet()\n",
        "model.load_state_dict(torch.load('squeezenet_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to predict the class of an individual image\n",
        "def predict_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/content/drive/MyDrive/MP_Dataset/AugmentedMI/MI(100).jpg\"\n",
        "predicted_class = predict_image(image_path)\n",
        "print(\"Predicted class:\", predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdHDy3_9apxl",
        "outputId": "06997daf-1848-4369-c60b-cc280c3be684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SqueezeNet 1_0 (LR 0.0001)\n",
        "Fold and accuracy"
      ],
      "metadata": {
        "id": "PFZesfVB2u19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model10 = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhkc0e8Cyi7g",
        "outputId": "c259a904-90a7-439b-eff9-47493fe68e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFSQ9FVvr8AV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a851ecbc-774f-4c63-dedc-f814616745bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.7042718948954243\n",
            "Epoch 2, Loss: 0.7889223848864183\n",
            "Epoch 3, Loss: 0.38302480025311647\n",
            "Epoch 4, Loss: 0.22635238088856816\n",
            "Epoch 5, Loss: 0.1464764740736366\n",
            "Accuracy for fold 1: 97.76595744680852%\n",
            "Epoch 1, Loss: 0.23204513158554496\n",
            "Epoch 2, Loss: 0.06698156676701096\n",
            "Epoch 3, Loss: 0.07572034698387142\n",
            "Epoch 4, Loss: 0.02792254113012597\n",
            "Epoch 5, Loss: 0.04788940068329655\n",
            "Accuracy for fold 2: 98.08510638297873%\n",
            "Epoch 1, Loss: 0.11748827453327791\n",
            "Epoch 2, Loss: 0.042527111818437054\n",
            "Epoch 3, Loss: 0.0033692027485017206\n",
            "Epoch 4, Loss: 0.019326885311545347\n",
            "Epoch 5, Loss: 0.07207109907916273\n",
            "Accuracy for fold 3: 98.5956746848502%\n",
            "Epoch 1, Loss: 0.05913737119063025\n",
            "Epoch 2, Loss: 0.028421234210450706\n",
            "Epoch 3, Loss: 0.004344418082540623\n",
            "Epoch 4, Loss: 0.010415682642522534\n",
            "Epoch 5, Loss: 0.004407635152709107\n",
            "Accuracy for fold 4: 96.91489361702128%\n",
            "Epoch 1, Loss: 0.05093581067838047\n",
            "Epoch 2, Loss: 0.027570577507654574\n",
            "Epoch 3, Loss: 0.008018506460618899\n",
            "Epoch 4, Loss: 0.015425268471696686\n",
            "Epoch 5, Loss: 0.036880746212133886\n",
            "Accuracy for fold 5: 99.78723440425532%\n",
            "Average accuracy: 98.22977330718281%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "from torchvision.datasets.folder import default_loader\n",
        "\n",
        "accuracies = []\n",
        "best_accuracy = 0.0\n",
        "best_model = None\n",
        "\n",
        "# Step 1: Define necessary transformations for your data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "X_paths = np.array([image_path for image_path, _ in data])\n",
        "y = np.array([label for _, label in data])\n",
        "\n",
        "\n",
        "# Step 3: Define K-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# Step 4: Iterate through the folds\n",
        "for fold, (train_indices, test_indices) in enumerate(kf.split(X_paths)):\n",
        "    X_train_paths, X_test_paths = X_paths[train_indices], X_paths[test_indices]\n",
        "    y_train, y_test = y[train_indices], y[test_indices]\n",
        "\n",
        "    # Step 5: Define custom dataset and data loaders\n",
        "    class CustomDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, X_paths, y, transform=None):\n",
        "            self.X_paths = X_paths\n",
        "            self.y = y\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.X_paths)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            img_path = self.X_paths[idx]\n",
        "            image = default_loader(img_path)\n",
        "            label = self.y[idx]\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "    train_dataset = CustomDataset(X_train_paths, y_train, transform=transform)\n",
        "    test_dataset = CustomDataset(X_test_paths, y_test, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Step 6: Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model10.parameters(), lr=0.0001)\n",
        "\n",
        "    # Step 7: Train the model\n",
        "    for epoch in range(5):  # Example: 5 epochs\n",
        "        model10.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model10(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Step 8: Evaluate the model\n",
        "    model10.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model10(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100*(correct / total)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Accuracy for fold {fold+1}: {accuracy}%\")\n",
        "\n",
        "    # Step 9: Save the best model\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model10\n",
        "\n",
        "# Step 10: Calculate and print average accuracy\n",
        "avg_accuracy = np.mean(accuracies)\n",
        "print(f\"Average accuracy: {avg_accuracy}%\")\n",
        "\n",
        "flatten_train_features = []\n",
        "flatten_test_features = []\n",
        "for inputs, _ in train_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = best_model(inputs)\n",
        "    flatten_train_features.extend(outputs.cpu().numpy())\n",
        "for inputs, _ in test_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = best_model(inputs)\n",
        "    flatten_test_features.extend(outputs.cpu().numpy())\n",
        "\n",
        "flatten_train_features = np.array(flatten_train_features)\n",
        "flatten_test_features = np.array(flatten_test_features)\n",
        "\n",
        "# Step 12: Save the best model for later use\n",
        "torch.save(best_model.state_dict(), \"best_model2.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifier: Navie Bayes,Random Forest,SVM,KNN,DT"
      ],
      "metadata": {
        "id": "DO8-RIjP2UMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torchvision.models import squeezenet1_0\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the SqueezeNet model\n",
        "model = squeezenet1_0(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define dataset paths and labels\n",
        "dataset_paths = np.array([image_path for image_path, _ in data])  # Paths to your dataset images\n",
        "labels = np.array([label for _, label in data])  # Corresponding labels for the dataset images\n",
        "\n",
        "# Extract features function\n",
        "def extract_features(model, image):\n",
        "    features = model(image.unsqueeze(0))\n",
        "    flattened_features = features.flatten().detach().numpy()\n",
        "    return flattened_features\n",
        "\n",
        "# Flatten features and collect labels\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for image_path, label in zip(dataset_paths, labels):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    flattened_features = extract_features(model.features, image)\n",
        "    X.append(flattened_features)\n",
        "    y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "classifiers = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    for classifier_name, classifier in classifiers.items():\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{classifier_name} Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVicvCDGnU6y",
        "outputId": "44bfb88a-cb61-46d9-ccef-dedd6a2a162e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "SVM Accuracy: 0.8012\n",
            "Decision Tree Accuracy: 0.7965\n",
            "Random Forest Accuracy: 0.8593\n",
            "KNN Accuracy: 0.7034\n",
            "Naive Bayes Accuracy: 0.6563\n",
            "Fold 2\n",
            "SVM Accuracy: 0.8625\n",
            "Decision Tree Accuracy: 0.8136\n",
            "Random Forest Accuracy: 0.8961\n",
            "KNN Accuracy: 0.7392\n",
            "Naive Bayes Accuracy: 0.6609\n",
            "Fold 3\n",
            "SVM Accuracy: 0.8961\n",
            "Decision Tree Accuracy: 0.8528\n",
            "Random Forest Accuracy: 0.9356\n",
            "KNN Accuracy: 0.7546\n",
            "Naive Bayes Accuracy: 0.6747\n",
            "Fold 4\n",
            "SVM Accuracy: 0.9105\n",
            "Decision Tree Accuracy: 0.8785\n",
            "Random Forest Accuracy: 0.9401\n",
            "KNN Accuracy: 0.8098\n",
            "Naive Bayes Accuracy: 0.7032\n",
            "Fold 5\n",
            "SVM Accuracy: 0.9436\n",
            "Decision Tree Accuracy: 0.8840\n",
            "Random Forest Accuracy: 0.9613\n",
            "KNN Accuracy: 0.8205\n",
            "Naive Bayes Accuracy: 0.7103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2TTeziwXCPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}